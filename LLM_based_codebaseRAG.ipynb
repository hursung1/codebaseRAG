{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70035dd",
   "metadata": {},
   "source": [
    "# 구현해야 할 것\n",
    "1. 업로드 한 코드의 파일 tree를 추출하는 것\n",
    "2. LLM을 이용해 봐야 할 코드 파일 후보를 추림\n",
    "3. 각 코드 별로 답변 생성에 적합한지 판단\n",
    "4. 만약 다른 코드를 찾아봐야 한다면, 다시 파일트리를 주고 판단하게 하든, 아니면 import 문으로 판단하게 하든 새롭게 봐야 할 파일을 정해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4155fffa",
   "metadata": {},
   "source": [
    "### 파일 tree 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ee4396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import utils\n",
    "import requests\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "language_map = {\n",
    "    \".py\": \"python\",\n",
    "    \".js\": \"javascript\",\n",
    "    \".ts\": \"typescript\",\n",
    "    \".java\": \"java\",\n",
    "    \".cpp\": \"cpp\",\n",
    "    \".c\": \"c\",\n",
    "    \".h\": \"c\",\n",
    "    \".hpp\": \"cpp\",\n",
    "    \".html\": \"html\",\n",
    "    \".css\": \"css\",\n",
    "    \".json\": \"json\",\n",
    "    \".yaml\": \"yaml\",\n",
    "    \".yml\": \"yaml\",\n",
    "    \".md\": \"markdown\",\n",
    "    \".txt\": \"text\",\n",
    "}\n",
    "codebase_path = \"code_snippets/trl-main\"\n",
    "\n",
    "if codebase_path.endswith(\"zip\"):\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    # subprocess.call()\n",
    "    subprocess.run(\n",
    "        [\"unzip\", \"-q\", codebase_path, \"-d\", temp_dir],\n",
    "        check=True,  # 실패 시 CalledProcessError 발생\n",
    "    )\n",
    "    codebase_path = temp_dir\n",
    "\n",
    "filetree = {} # key: file_path, value: is_unseen\n",
    "for path, dirs, files in os.walk(codebase_path):\n",
    "    for file in files:\n",
    "        ext = file.split(\".\")[-1]\n",
    "        if not f\".{ext}\" in language_map.keys(): continue\n",
    "        filetree[f\"{path}/{file}\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37d2620",
   "metadata": {},
   "source": [
    "### 파일 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d73a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_file(user_query: str, filetree: dict):\n",
    "    unseen_files = []\n",
    "    for file, is_unseen in filetree.items():\n",
    "        if is_unseen:\n",
    "            unseen_files.append(file)\n",
    "\n",
    "    prompt = \"\"\"### 할 일\n",
    "- 사용자 질의와 파일 목록을 보고, 어떤 파일에서 사용자 질의를 만족할 수 있는 내용이 있을지 파일 경로 및 이름을 통해 유추하고 판단한다.\n",
    "\n",
    "### 유의사항\n",
    "- 사용자 질의 의도를 만족할 수 있는 파일의 경로 최대 5가지 만을, python list 형식으로 return 한다.\n",
    "- 다른 부연설명이나 추가적인 설명은 절대로 return하지 않는다.\n",
    "- \"파일 목록\"에 없는 내용을 절대로 return하지 않는다.\n",
    "\"\"\"\n",
    "    user_input = f\"\"\"- 사용자 질의: {user_query}\\n- 파일 목록: {unseen_files}\"\"\"\n",
    "\n",
    "    llm_out = utils.call_llm(prompt=prompt, user_input=user_input)[\"content\"].strip()\n",
    "    # print(llm_out)\n",
    "    try:\n",
    "        selected_files = json.loads(llm_out.replace(\"'\", \"\\\"\"))\n",
    "    except:\n",
    "        if \"```\" in llm_out:\n",
    "            llm_out = llm_out.strip(\"```\").strip(\"python\")\n",
    "            selected_files = json.loads(llm_out.replace(\"'\", \"\\\"\"))\n",
    "    for file in selected_files:\n",
    "        filetree[file] = False\n",
    "        \n",
    "    return selected_files\n",
    "    print(llm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448f4eb",
   "metadata": {},
   "source": [
    "### 파일 내용 보고 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498e45f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_answer(user_query: str, selected_files: list):\n",
    "    \"\"\"\n",
    "    Get user's query and file paths(selected_files). \n",
    "    LLM traverses files and create candidate answers if content of the file matches or is related to user's query.\n",
    "\n",
    "    Finally, gather and summarize candidate answers into **final_answer**.\n",
    "    \"\"\"\n",
    "    \n",
    "    identify_code_prompt = \"\"\"### 할 일\n",
    "- 주어진 사용자 질의와 코드 내용을 보고, 사용자의 질의 의도를 충분히 만족할 수 있는 내용인지 판단한 후, 만족할 수 있다면 해당하는 부분의 코드 내용을 추출한다.\n",
    "\n",
    "### 유의사항\n",
    "- 사용자는 주어진 코드에 관한 내용을 물어보는 경우가 일반적이므로, general한 함수나 클래스에 대해서는 중요도를 낮춰야 한다.\n",
    "- 부연설명이나 이유 등의 내용은 중요도를 낮춰야 한다.\n",
    "- 충분히 만족할 수 없는 내용인 경우, 무조건 \"pass\"만을 출력한다.\n",
    "- 답변은 무조건 한국어로 출력한다.\n",
    "\"\"\"\n",
    "\n",
    "    answer_candidates = []\n",
    "\n",
    "    for file in selected_files:\n",
    "        with open(file, \"r\") as f:\n",
    "            codes = f.read()\n",
    "\n",
    "        query_code_user_input = f\"\"\"- 사용자 질의: {user_query}\\n- 코드 내용: {codes}\"\"\"\n",
    "        codeidf_llm_out = utils.call_llm(prompt=identify_code_prompt, user_input=query_code_user_input)[\"content\"].strip()\n",
    "        answer_candidates.append(codeidf_llm_out)\n",
    "        \n",
    "    # 최종 답변 생성\n",
    "    final_ans_prompt = \"\"\"### 할 일\n",
    "- 사용자 질의와 주어진 파일 별 답변 생성 내용을 보고, 사용자 질의에 맞는 내용만을 선택해 적절한 답변을 출력한다.\n",
    "\n",
    "### 유의사항\n",
    "- 만약 모든 \"파일 별 답변 생성 내용\"이 \"pass\" 라면 \"pass\" 만을 그대로 출력한다. 이 경우 부연설명이나 이유 등의 내용은 중요도를 낮춰야 한다.\n",
    "- 답변은 무조건 한국어로 출력한다.\n",
    "\"\"\"\n",
    "    final_ans_user_input = f\"\"\"- 사용자 질의: {user_query}\\n- 파일 별 답변 생성 내용: {answer_candidates}\"\"\"\n",
    "    \n",
    "    final_answer = utils.call_llm(prompt=final_ans_prompt, user_input=final_ans_user_input)[\"content\"]\n",
    "    return answer_candidates, final_answer\n",
    "    print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901e76e",
   "metadata": {},
   "source": [
    "## 사용자 질의문 등 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ce3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as f:\n",
    "    configs = json.load(f)\n",
    "\n",
    "user_query = configs[\"query\"]\n",
    "final_answer = \"pass\" # temporaral initialization\n",
    "max_count = 5\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1910a0",
   "metadata": {},
   "source": [
    "### Query decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0611c4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"orpo trainer의 코드 구현 방식은 무엇인가?\", \"dpo trainer의 코드 구현 방식은 무엇인가?\", \"orpo trainer와 dpo trainer의 코드 구현의 차이점은 무엇인가?\"]\n"
     ]
    }
   ],
   "source": [
    "subquery_decomp_prompt = configs[\"prompts\"][\"subquery_prompt\"]\n",
    "\n",
    "user_input = f\"### 사용자 질의문\\n- {user_query}\"\n",
    "\n",
    "llm_out = utils.call_llm(prompt=subquery_decomp_prompt, user_input=user_input)\n",
    "print(llm_out[\"content\"])\n",
    "\n",
    "atomic_queries = json.loads(llm_out[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1c9196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as f:\n",
    "    configs = json.load(f)\n",
    "    prompts = configs[\"prompts\"]\n",
    "\n",
    "answer_candidates = []\n",
    "final_answer_per_queries = {}\n",
    "\n",
    "with open(\"subquery_log_file.md\", \"a+\") as f:\n",
    "    f.write(f\"# Original Query \\n{user_query}\\n\")\n",
    "    for i, q in enumerate(atomic_queries):\n",
    "        answer_candidates = []\n",
    "        selected_files = utils.select_file(\n",
    "            prompts=prompts, \n",
    "            user_query=q, \n",
    "            filetree=filetree, \n",
    "            show_past_docs=False)\n",
    "        \n",
    "        answer_candidates, final_answer = utils.create_answer(\n",
    "            prompts=prompts,\n",
    "            user_query=q,\n",
    "            selected_files=selected_files,\n",
    "            answer_candidates=answer_candidates,\n",
    "            handle_code_indiv=False\n",
    "        )\n",
    "\n",
    "        final_answer_per_queries[q] = final_answer\n",
    "    \n",
    "        f.write(f\"## Sub Query {i}\\n{q}\\n\\n\")\n",
    "        f.write(f\"## Answer\\n{final_answer}\\n\\n\")\n",
    "\n",
    "#     combine_prompt = \"\"\"### 할 일\n",
    "# - 사용자 질의, 그 질의를 최소 단위로 분리한 질의, 최소 단위 질의에 대한 답변 목록을 이용하여, 사용자 질의 의도에 맞도록 이를 조합하여 최종 답변을 출력한다.\n",
    "\n",
    "# ### 출력 내용\n",
    "# - 최종 답변: 사용자 질의 의도에 맞으면서, 최소 단위 질의에 대한 답변 내용을   \n",
    "# \"\"\"\n",
    "#     user_input = f\"### 사용자 질의\\n- {user_query}\\n### 답변 목록\\n{str(final_answer)}\\n\"\n",
    "\n",
    "#     llm_out = utils.call_llm(prompt=combine_prompt, user_input=user_input)[\"content\"]\n",
    "#     f.write(f\"## Final Answer\\n{llm_out}\\n\")\n",
    "#     f.write(f\"### Args \\n- codebase_path: {codebase_path}\\n- sub query decomp: True\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c443e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 사용자 질의\n",
      "- orpo trainer와 dpo trainer의 코드 구현이 어떤 차이가 있는지 구체적으로 설명해줘\n",
      "### 답변 목록\n",
      "{'orpo trainer의 코드 구현 방식은 무엇인가?': '### orpo trainer의 코드 구현 방식은 무엇인가?\\n\\norpo trainer는 `Trainer` 클래스를 상속받아 구현된 클래스입니다. orpo trainer는 주어진 모델과 데이터셋을 사용하여 orpo 알고리즘을 적용한 학습을 수행합니다. 주요 구성 요소는 다음과 같습니다.\\n\\n```python\\nclass ORPOTrainer(Trainer):\\n    def __init__(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, ...):\\n        # 초기화 코드\\n        super().__init__(model, args, data_collator, train_dataset, eval_dataset, processing_class, ...)\\n        \\n    def tokenize_row(self, feature):\\n        # 데이터 토큰화 코드\\n        pass\\n    \\n    def concatenated_inputs(self, batch):\\n        # 입력 배치 데이터 처리 코드\\n        pass\\n    \\n    def odds_ratio_loss(self, policy_chosen_logps, policy_rejected_logps):\\n        # orpo 손실 함수 계산 코드\\n        pass\\n    \\n    def concatenated_forward(self, model, batch):\\n        # 모델의 순전파 코드\\n        pass\\n    \\n    def get_batch_loss_metrics(self, model, batch):\\n        # 배치 손실 및 메트릭 계산 코드\\n        pass\\n    \\n    def compute_loss(self, model, inputs):\\n        # 손실 계산 코드\\n        pass\\n```\\n\\norpo trainer는 `Trainer` 클래스의 기능을 확장하여 orpo 알고리즘을 구현합니다. 주요 기능은 데이터 토큰화, 입력 배치 데이터 처리, orpo 손실 함수 계산, 모델의 순전파, 배치 손실 및 메트릭 계산, 손실 계산 등입니다.\\n\\n### 해당 코드에 대한 설명\\n\\norpo trainer는 orpo 알고리즘을 적용하여 모델을 학습시키기 위한 클래스입니다. orpo 알고리즘은 선호도 학습을 위한 방법으로, 모델이 선호하는 응답과 그렇지 않은 응답 간의 차이를 학습하도록 합니다. orpo trainer는 주어진 모델과 데이터셋을 사용하여 orpo 알고리즘을 적용한 학습을 수행하며, 학습 과정에서 다양한 메트릭을 계산하고 기록합니다. 주요 구성 요소는 데이터 토큰화, 입력 배치 데이터 처리, orpo 손실 함수 계산, 모델의 순전파, 배치 손실 및 메트릭 계산, 손실 계산 등입니다.', 'dpo trainer의 코드 구현 방식은 무엇인가?': '### 코드 구현 방식\\nDPOTrainer는 주어진 모델과 참조 모델을 사용하여 DPO(Direct Preference Optimization) 알고리즘을 구현하는 클래스입니다. 이 클래스는 Hugging Face의 Trainer 클래스를 상속받아 확장된 기능을 제공합니다.\\n\\n```python\\nclass DPOTrainer(Trainer):\\n    def __init__(self, model, ref_model, args, ...):\\n        # 모델과 참조 모델 초기화\\n        self.model = model\\n        self.ref_model = ref_model\\n        # DPOConfig 객체를 통해 전달된 인자 처리\\n        self.args = args\\n        # 데이터 콜레이터 및 데이터셋 준비\\n        self.data_collator = data_collator\\n        self.train_dataset = train_dataset\\n        self.eval_dataset = eval_dataset\\n        # 모델 초기화 및 기타 설정\\n        ...\\n```\\n\\n### 설명\\nDPOTrainer는 DPO 알고리즘을 사용하여 언어 모델을 선호도 데이터에 맞게 미세 조정하는 데 사용됩니다. 이 클래스는 모델과 참조 모델을 입력으로 받아 초기화하며, DPOConfig 객체를 통해 다양한 하이퍼파라미터를 설정할 수 있습니다. 주요 기능으로는 DPO 손실 함수 계산, 모델 및 참조 모델의 로그 확률 계산, 그리고 학습 및 평가를 위한 데이터 로더 생성이 있습니다. 또한, PEFT(Parameter-Efficient Fine-Tuning)와 같은 기술을 지원하여 효율적인 모델 미세 조정을 가능하게 합니다.', 'orpo trainer와 dpo trainer의 코드 구현의 차이점은 무엇인가?': '### orpo trainer와 dpo trainer의 코드 구현의 차이점\\n\\norpo trainer와 dpo trainer의 주요 차이점은 사용하는 손실 함수와 모델 구성에 있다.\\n\\n#### 코드 비교\\n\\n1. **DPOConfig vs ORPOConfig**:\\n   - `DPOConfig`는 `TrainingArguments`를 상속받아 DPO 학습을 위한 설정을 정의한다. 다양한 손실 함수(`loss_type`)를 지원하며, `beta`, `f_divergence_type` 등의 매개변수를 포함한다.\\n   - `ORPOConfig` 역시 `TrainingArguments`를 상속받지만, ORPO 학습을 위해 설계되었으며 `beta` 매개변수를 통해 ORPO 손실 함수의 가중치를 조절한다.\\n\\n2. **DPOTrainer vs ORPOTrainer**:\\n   - `DPOTrainer`는 DPO 알고리즘을 구현한 클래스로, 모델, 참조 모델, 보상 모델 또는 판정자(judge) 등을 인자로 받아 학습을 진행한다. 다양한 손실 함수를 지원하며, 온라인 및 오프라인 DPO 학습을 모두 지원한다.\\n   - `ORPOTrainer`는 ORPO 알고리즘을 구현한 클래스로, `DPOTrainer`와 유사하지만 ORPO 손실 함수를 사용한다. (주어진 파일에는 `ORPOTrainer`의 구체적인 구현이 포함되어 있지 않지만, `ORPOConfig`를 통해 ORPO 학습을 위한 설정이 가능함을 알 수 있다.)\\n\\n#### 주요 차이점 요약\\n\\n- **손실 함수**: DPO는 다양한 손실 함수(`sigmoid`, `ipo`, `exo_pair` 등)를 지원하지만, ORPO는 ORPO 손실 함수를 사용한다.\\n- **모델 구성 및 설정**: 두 trainer 모두 모델과 참조 모델을 사용할 수 있지만, ORPO는 추가적인 구성 요소나 설정을 필요로 할 수 있다.\\n\\n### 예시 코드\\n\\n#### DPO Trainer 예시\\n```python\\nfrom trl import DPOConfig, DPOTrainer\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# 모델 및 토크나이저 로드\\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\\n\\n# DPOConfig 설정\\ntraining_args = DPOConfig(output_dir=\"Qwen2-0.5B-DPO\", learning_rate=5.0e-7)\\n\\n# DPOTrainer 초기화\\ntrainer = DPOTrainer(model, ref_model, args=training_args, train_dataset=train_dataset, processing_class=tokenizer)\\ntrainer.train()\\n```\\n\\n#### ORPO Trainer 예시 (가정)\\n```python\\nfrom trl import ORPOConfig, ORPOTrainer\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# 모델 및 토크나이저 로드\\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\\n\\n# ORPOConfig 설정\\ntraining_args = ORPOConfig(output_dir=\"Qwen2-0.5B-ORPO\", learning_rate=5.0e-7, beta=0.1)\\n\\n# ORPOTrainer 초기화 (구현이 제공되지 않아 가정)\\ntrainer = ORPOTrainer(model, ref_model, args=training_args, train_dataset=train_dataset, processing_class=tokenizer)\\ntrainer.train()\\n```'}\n",
      "\n",
      "### 최종 답변\n",
      "orpo trainer와 dpo trainer의 코드 구현의 주요 차이점은 다음과 같습니다.\n",
      "\n",
      "1. **손실 함수**: \n",
      "   - DPO trainer는 다양한 손실 함수(`sigmoid`, `ipo`, `exo_pair` 등)를 지원합니다.\n",
      "   - ORPO trainer는 ORPO 손실 함수를 사용합니다.\n",
      "\n",
      "2. **모델 구성 및 설정**:\n",
      "   - 두 trainer 모두 모델과 참조 모델을 사용할 수 있습니다.\n",
      "   - ORPO trainer는 추가적인 구성 요소나 설정을 필요로 할 수 있습니다. 예를 들어, `ORPOConfig`를 통해 ORPO 학습을 위한 설정을 정의하며, `beta` 매개변수를 통해 ORPO 손실 함수의 가중치를 조절합니다.\n",
      "\n",
      "3. **구현 클래스 및 설정**:\n",
      "   - `DPOTrainer`는 DPO 알고리즘을 구현한 클래스로, 모델, 참조 모델, 보상 모델 또는 판정자(judge) 등을 인자로 받아 학습을 진행합니다.\n",
      "   - `ORPOTrainer`는 ORPO 알고리즘을 구현한 클래스로, `DPOTrainer`와 유사하지만 ORPO 손실 함수를 사용합니다.\n",
      "\n",
      "예시 코드를 통해 두 trainer의 사용 방식을 비교할 수 있습니다.\n",
      "\n",
      "#### DPO Trainer 예시\n",
      "```python\n",
      "from trl import DPOConfig, DPOTrainer\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "\n",
      "# 모델 및 토크나이저 로드\n",
      "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
      "\n",
      "# DPOConfig 설정\n",
      "training_args = DPOConfig(output_dir=\"Qwen2-0.5B-DPO\", learning_rate=5.0e-7)\n",
      "\n",
      "# DPOTrainer 초기화\n",
      "trainer = DPOTrainer(model, ref_model, args=training_args, train_dataset=train_dataset, processing_class=tokenizer)\n",
      "trainer.train()\n",
      "```\n",
      "\n",
      "#### ORPO Trainer 예시\n",
      "```python\n",
      "from trl import ORPOConfig, ORPOTrainer\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "\n",
      "# 모델 및 토크나이저 로드\n",
      "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
      "\n",
      "# ORPOConfig 설정\n",
      "training_args = ORPOConfig(output_dir=\"Qwen2-0.5B-ORPO\", learning_rate=5.0e-7, beta=0.1)\n",
      "\n",
      "# ORPOTrainer 초기화\n",
      "trainer = ORPOTrainer(model, ref_model, args=training_args, train_dataset=train_dataset, processing_class=tokenizer)\n",
      "trainer.train()\n",
      "```\n",
      "\n",
      "이처럼 orpo trainer와 dpo trainer는 손실 함수와 모델 구성 및 설정에서 차이점을 보입니다.\n"
     ]
    }
   ],
   "source": [
    "combine_prompt = \"\"\"### 할 일\n",
    "- 사용자 질의, 그 질의를 최소 단위로 분리한 질의, 최소 단위 질의에 대한 답변 목록을 이용하여, 사용자 질의 의도에 맞도록 최종 답변을 출력한다.\n",
    "\n",
    "### 출력 내용\n",
    "- 최종 답변: 최소 단위 질의에 대한 답변 내용을 기반으로 한 사용자 질의 의도에 맞는 답변\n",
    "\"\"\"\n",
    "user_input = f\"### 사용자 질의\\n- {user_query}\\n### 답변 목록\\n{str(final_answer_per_queries)}\\n\"\n",
    "\n",
    "print(user_input)\n",
    "\n",
    "llm_out = utils.call_llm(prompt=combine_prompt, user_input=user_input)[\"content\"]\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a90a96d",
   "metadata": {},
   "source": [
    "### 그냥 한 번 돌려보고 싶을때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c6ac37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a trainer code that uses REINFORCE loss, you can define a custom loss function and pass it to the `Trainer` class using the `compute_loss_func` argument.\n",
      "\n",
      "```python\n",
      "def reinforce_loss(outputs, labels, rewards):\n",
      "    log_probs = outputs[0]\n",
      "    loss = -torch.mean(rewards * log_probs)\n",
      "    return loss\n",
      "\n",
      "# Example usage\n",
      "trainer = Trainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    compute_loss_func=lambda outputs, labels, num_items_in_batch: reinforce_loss(outputs, labels, rewards)\n",
      ")\n",
      "```\n",
      "\n",
      "This code defines a custom `reinforce_loss` function that calculates the REINFORCE loss based on the log probabilities and rewards. The `Trainer` class is then initialized with this custom loss function using the `compute_loss_func` argument. Note that the exact implementation details may vary based on the specific requirements of your model and the structure of your outputs and labels.\n"
     ]
    }
   ],
   "source": [
    "# First, find appropriate file from filetree with llm\n",
    "selected_files = select_file(user_query=user_query, filetree=filetree)\n",
    "\n",
    "# Then, input selected file paths and user's query to LLM again\n",
    "answer_candidates, final_answer = create_answer(user_query=user_query, selected_files=selected_files)\n",
    "\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db0fdc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected files: ['/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmp86pcatv7/transformers-main/src/transformers/trainer.py']\n",
      "Answer candidates: ['### Thought Process and Reasoning\\n\\nTo address the user\\'s query about overriding the `Trainer` class to create a custom trainer, we need to understand the structure and functionality of the `Trainer` class in the Hugging Face Transformers library.\\n\\n1. **Understanding the `Trainer` Class**: The `Trainer` class is a central component of the Hugging Face Transformers library, designed to simplify the training and evaluation of transformer-based models. It provides a lot of functionality out of the box, including support for distributed training, mixed precision, and various optimization algorithms.\\n\\n2. **Identifying the Need for Customization**: The user wants to create a custom trainer by overriding the `Trainer` class. This implies that they need to modify or extend the existing functionality of the `Trainer` to suit their specific requirements.\\n\\n3. **Locating Relevant Methods for Override**: To create a custom trainer, one needs to identify the methods within the `Trainer` class that can be overridden. Key methods include `create_optimizer_and_scheduler`, `get_train_dataloader`, `get_eval_dataloader`, `training_step`, and `compute_loss`, among others. The user should focus on the methods that are relevant to their customization needs.\\n\\n4. **Example of Overriding the `Trainer` Class**: A common reason to override the `Trainer` class is to customize the optimization process. For instance, a user might want to use a different optimizer or learning rate scheduler than those provided by default. This can be achieved by subclassing `Trainer` and overriding the `create_optimizer_and_scheduler` method.\\n\\n### Solution\\n\\nTo override the `Trainer` class and create a custom trainer, you can follow this example:\\n\\n```python\\nfrom transformers import Trainer, TrainingArguments\\n\\nclass CustomTrainer(Trainer):\\n    def create_optimizer_and_scheduler(self, num_training_steps: int):\\n        # Custom implementation for creating the optimizer and scheduler\\n        # This is just an example; you can modify it according to your needs\\n        super().create_optimizer_and_scheduler(num_training_steps)\\n        # Additional custom logic can be added here\\n\\n# Usage\\ntraining_args = TrainingArguments(\\n    output_dir=\"./results\",\\n    num_train_epochs=3,\\n    per_device_train_batch_size=16,\\n    per_device_eval_batch_size=64,\\n    warmup_steps=500,\\n    weight_decay=0.01,\\n    logging_dir=\"./logs\",\\n)\\n\\n# Assuming you have a model and dataset ready\\nmodel = ...  # Your model\\ntrain_dataset = ...  # Your training dataset\\neval_dataset = ...  # Your evaluation dataset\\n\\ntrainer = CustomTrainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=train_dataset,\\n    eval_dataset=eval_dataset,\\n)\\n\\ntrainer.train()\\n```\\n\\n### Code Extraction\\n\\nThe relevant code for overriding the `Trainer` class involves defining a subclass (`CustomTrainer`) and overriding the necessary methods. The example above demonstrates how to create a `CustomTrainer` class that inherits from `Trainer` and overrides the `create_optimizer_and_scheduler` method.\\n\\n```python\\nclass CustomTrainer(Trainer):\\n    def create_optimizer_and_scheduler(self, num_training_steps: int):\\n        # Custom logic here\\n        pass\\n```\\n\\nThis code snippet is the core of creating a custom trainer. You can further customize other methods as needed based on your specific requirements.\\n\\n### Answer\\n\\nTo create a custom trainer by overriding the `Trainer` class, you should subclass `Trainer` and override the methods that need customization. For example, you can override `create_optimizer_and_scheduler` to use a custom optimizer or scheduler. \\n\\n```python\\nclass CustomTrainer(Trainer):\\n    def create_optimizer_and_scheduler(self, num_training_steps: int):\\n        # Custom implementation\\n        super().create_optimizer_and_scheduler(num_training_steps)\\n        # Add custom logic here\\n```']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Selected files: {selected_files}\")\n",
    "print(f\"Answer candidates: {answer_candidates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9d146f",
   "metadata": {},
   "source": [
    "## 만약 답변이 만족스럽지 못하다면?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ae014",
   "metadata": {},
   "source": [
    "### 방법 1) 파일 재검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c06b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "while final_answer == \"pass\" and count < max_count:\n",
    "    # First, input them into LLM to select appropriate ones\n",
    "    selected_files = select_file(user_query=user_query, filetree=filetree)\n",
    "\n",
    "    # Then, input selected files' paths and user's query to LLM to get final answer\n",
    "    final_answer = create_answer(user_query=user_query, selected_files=selected_files)\n",
    "\n",
    "    # This is for loop limitation\n",
    "    count += 1\n",
    "\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ffc834",
   "metadata": {},
   "source": [
    "### 방법 2) 파일 내 import 문에서 파일 타고 들어가기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd087116",
   "metadata": {},
   "source": [
    "#### Build with AST module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15191eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "class Analyzer(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self.stats = {\"import\": [], \"from\": [], \"class\": [], \"function\": []}\n",
    "\n",
    "    def visit_ImportFrom(self, node):\n",
    "        for alias in node:\n",
    "            self.stats[\"from\"].append(alias.name)\n",
    "        \n",
    "    def visit_Import(self, node):\n",
    "        for alias in node.names:\n",
    "            self.stats[\"import\"].append(alias.name)\n",
    "        \n",
    "    def visit_ClassDef(self, node):\n",
    "        self.stats[\"class\"].append(node.name)\n",
    "        self.generic_visit(node)\n",
    "    \n",
    "    def visit_FunctionDef(self, node):\n",
    "        self.stats[\"function\"].append(node.name)\n",
    "        self.generic_visit(node)\n",
    "    \n",
    "\n",
    "def find_pckg(package_name: str, module_names: list, root_dir: str):\n",
    "    \"\"\"\n",
    "    현재 package_name이 dir, module_names가 함수명/클래스명일 때 해당 파일을 추가하지 못하는 문제가 있음\n",
    "    \"\"\"\n",
    "    connected_files = []\n",
    "    for path, dirs, files in os.walk(root_dir):\n",
    "        if package_name:\n",
    "            if package_name in dirs: # if package name exists and is in directory: find module_name in dir\n",
    "                connected_files.extend([os.path.join(path, package_name, f\"{module}.py\") for module in module_names if f\"{module}.py\" in files])\n",
    "            elif f\"{package_name}.py\" in files: # if package name is file: add that file to connected_files\n",
    "                connected_files.append(os.path.join(path, f\"{package_name}.py\"))\n",
    "\n",
    "        else: # if not package_name: add all module_names to connected_files\n",
    "            for module in module_names:\n",
    "                if f\"{module}.py\" in files:\n",
    "                    connected_files.append(os.path.join(path, f\"{module}.py\"))\n",
    "    \n",
    "    return connected_files\n",
    "\n",
    "\n",
    "def get_linked_files(selected_files: list, filetree: dict):\n",
    "    linked_files_dict = defaultdict(list)\n",
    "    for file_path in selected_files:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            tree = ast.parse(f.read())\n",
    "\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.ImportFrom):\n",
    "                package_name = node.module\n",
    "                module_names = [name.name for name in node.names]\n",
    "\n",
    "            elif isinstance(node, ast.Import):\n",
    "                package_name = None\n",
    "                module_names = [name.name for name in node.names]\n",
    "\n",
    "            else: continue\n",
    "            \n",
    "            connected_files = find_pckg(package_name=package_name, module_names=module_names, root_dir=codebase_path)\n",
    "            linked_files_dict[file_path].extend(connected_files)\n",
    "            for file in connected_files:\n",
    "                filetree[file] = False\n",
    "\n",
    "    return linked_files_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434ce3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>,\n",
      "            {'/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/trainer.py': ['/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/configuration_utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/generation/configuration_utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/debug_utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/feature_extraction_sequence_utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/feature_extraction_utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/hyperparameter_search.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/image_processing_utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/modelcard.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/modeling_utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/optimization.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/processing_utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/pytorch_utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/tokenization_utils_base.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/trainer_callback.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/trainer_pt_utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/trainer_utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/training_args.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/examples/legacy/seq2seq/utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/onnx/utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/data/processors/utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/generation/utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/trainer_pt_utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/integrations/peft.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/integrations/accelerate.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/integrations/accelerate.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/trainer_pt_utils.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/integrations/peft.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/integrations/peft.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/utils/bitsandbytes.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/integrations/bitsandbytes.py',\n",
      "                                                                                                                            '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmpj7xia7yo/transformers-main/src/transformers/modeling_outputs.py']})\n"
     ]
    }
   ],
   "source": [
    "# First, find appropriate file from filetree with llm\n",
    "selected_files = select_file(user_query=user_query, filetree=filetree)\n",
    "\n",
    "# Then, input selected file paths and user's query to LLM again\n",
    "answer_candidates, final_answer = create_answer(user_query=user_query, selected_files=selected_files)\n",
    "\n",
    "print(final_answer)\n",
    "\n",
    "while final_answer == \"pass\": # 제대로 된 대답이 나올 때 까지 반복\n",
    "    selected_files = get_linked_files(selected_files=selected_files, filetree=filetree)\n",
    "    answer_candidates, final_answer = create_answer(user_query=user_query, selected_files=selected_files)\n",
    "    print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb92967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE PATH: /var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmphaprmlon/PlanGen-main/generator/generator.py\n",
      "[]\n",
      "FILE PATH: /var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmphaprmlon/PlanGen-main/content_planner/contentplanner.py\n",
      "['/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmprgxt_1uc/PlanGen-main/content_planner/dynamic_crf_layer.py', '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmprgxt_1uc/PlanGen-main/content_planner/utlis.py', '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmprgxt_1uc/PlanGen-main/generator/utlis.py']\n",
      "FILE PATH: /var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmphaprmlon/PlanGen-main/generator/utlis.py\n",
      "[]\n",
      "FILE PATH: /var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmphaprmlon/PlanGen-main/content_planner/utlis.py\n",
      "[]\n",
      "FILE PATH: /var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmphaprmlon/PlanGen-main/data/data_processing.py\n",
      "['/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmprgxt_1uc/PlanGen-main/generator/data_processing_funcs.py', '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmprgxt_1uc/PlanGen-main/data/data_processing_funcs.py', '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmprgxt_1uc/PlanGen-main/generator/text_processing_funcs.py', '/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/tmprgxt_1uc/PlanGen-main/data/text_processing_funcs.py']\n"
     ]
    }
   ],
   "source": [
    "# 구조 효율화 필요\n",
    "\n",
    "def get_associated_files(codebase_path: str, selected_files: list):\n",
    "    file_imports = {}\n",
    "    for file_path in selected_files:\n",
    "        # print(f\"FILE PATH: {file_path}\")\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        packages = []\n",
    "        for line in lines:\n",
    "            if not \"import\" in line: continue # import 문이 없으면: 되돌아감\n",
    "\n",
    "            keywords = line.strip().split()\n",
    "\n",
    "            # Pattern of \"\"\"import\"\"\"\n",
    "            # 1. import A (as B)\n",
    "            # 2. from A import B\n",
    "            # \"\"\"A\"\"\" can be \"file name\" or \"directory name\"\n",
    "            package_name = keywords[1]\n",
    "            if len(keywords) > 2 and keywords[2] == \"import\":\n",
    "                module_name = keywords[3]\n",
    "\n",
    "            for path, dirs, files in os.walk(codebase_path):\n",
    "                if f\"{package_name}.py\" in files: # 1. if package_name is file name\n",
    "                    packages.append(os.path.join(path, f\"{package_name}.py\"))\n",
    "                elif package_name in dirs: # 2. if package_name is directory name\n",
    "                    if module_name == \"*\": # 2-1. wildcard: get all files in directory\n",
    "                        for file in files:\n",
    "                            packages.append(os.path.join(path, package_name, f\"{file}.py\"))\n",
    "                    elif f\"{module_name}.py\" in files: # 2-2. if module_name is file name\n",
    "                        packages.append(os.path.join(path, package_name, f\"{module_name}.py\"))\n",
    "\n",
    "        # print(packages)\n",
    "        file_imports[file_path] = packages\n",
    "\n",
    "    return file_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64502a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seonghwan/miniconda3/envs/langchain/lib/python3.12/site-packages/executing/executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "/Users/seonghwan/miniconda3/envs/langchain/lib/python3.12/ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_associated_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m file_imports = get_associated_files(codebase_path=codebase_path, selected_files=selected_files)\n\u001b[32m      2\u001b[39m selected_files_asdf = []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, value \u001b[38;5;129;01min\u001b[39;00m file_imports.items():\n",
      "\u001b[31mNameError\u001b[39m: name 'get_associated_files' is not defined"
     ]
    }
   ],
   "source": [
    "file_imports = get_associated_files(codebase_path=codebase_path, selected_files=selected_files)\n",
    "selected_files_asdf = []\n",
    "for _, value in file_imports.items():\n",
    "    selected_files_asdf.extend(value)\n",
    "\n",
    "final_answer = create_answer(user_query=user_query, selected_files=selected_files_asdf)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b801f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "while final_answer == \"pass\" and count < max_count:\n",
    "    # Get unused files first\n",
    "    filetree_unused = [file_path for file_path, is_unseen in filetree.items() if is_unseen]\n",
    "\n",
    "    # Second, input them into LLM\n",
    "    selected_files = select_file(user_query=user_query, filetree=filetree_unused)\n",
    "\n",
    "    # Finally, input selected file paths and user's query to LLM again\n",
    "    final_answer = create_answer(user_query=user_query, selected_files=selected_files)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abe3cc3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.12/site-packages/numpy/core/__init__.py:24\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.12/site-packages/numpy/core/multiarray.py:10\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mfunctools\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.12/site-packages/numpy/core/overrides.py:8\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getargspec\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n\u001b[32m     12\u001b[39m ARRAY_FUNCTIONS = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mImportError\u001b[39m: dlopen(/Users/seonghwan/miniconda3/envs/langchain/lib/python3.12/site-packages/numpy/core/_multiarray_umath.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libgfortran.5.dylib\n  Referenced from: <C435405B-BFF1-399F-A1EC-BD43418F546A> /Users/seonghwan/miniconda3/envs/langchain/lib/libopenblas.0.dylib\n  Reason: tried: '/Users/seonghwan/miniconda3/envs/langchain/lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/seonghwan/miniconda3/envs/langchain/lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/seonghwan/miniconda3/envs/langchain/lib/python3.12/site-packages/numpy/core/../../../../libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/seonghwan/miniconda3/envs/langchain/lib/python3.12/site-packages/numpy/core/../../../../libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/seonghwan/miniconda3/envs/langchain/bin/../lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/seonghwan/miniconda3/envs/langchain/bin/../lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/usr/local/lib/libgfortran.5.dylib' (no such file), '/usr/lib/libgfortran.5.dylib' (no such file, not in dyld cache)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.12/site-packages/numpy/__init__.py:130\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__config__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show \u001b[38;5;28;01mas\u001b[39;00m show_config\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.12/site-packages/numpy/__config__.py:4\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     __cpu_features__,\n\u001b[32m      6\u001b[39m     __cpu_baseline__,\n\u001b[32m      7\u001b[39m     __cpu_dispatch__,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mshow\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.12/site-packages/numpy/core/__init__.py:50\u001b[39m\n\u001b[32m     27\u001b[39m     msg = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m \u001b[33m\"\"\"\u001b[39m % (sys.version_info[\u001b[32m0\u001b[39m], sys.version_info[\u001b[32m1\u001b[39m], sys.executable,\n\u001b[32m     49\u001b[39m         __version__, exc)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.12 from \"/Users/seonghwan/miniconda3/envs/langchain/bin/python\"\n  * The NumPy version is: \"1.26.4\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Users/seonghwan/miniconda3/envs/langchain/lib/python3.12/site-packages/numpy/core/_multiarray_umath.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libgfortran.5.dylib\n  Referenced from: <C435405B-BFF1-399F-A1EC-BD43418F546A> /Users/seonghwan/miniconda3/envs/langchain/lib/libopenblas.0.dylib\n  Reason: tried: '/Users/seonghwan/miniconda3/envs/langchain/lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/seonghwan/miniconda3/envs/langchain/lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/seonghwan/miniconda3/envs/langchain/lib/python3.12/site-packages/numpy/core/../../../../libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/seonghwan/miniconda3/envs/langchain/lib/python3.12/site-packages/numpy/core/../../../../libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/seonghwan/miniconda3/envs/langchain/bin/../lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/Users/seonghwan/miniconda3/envs/langchain/bin/../lib/libgfortran.5.dylib' (duplicate LC_RPATH '@loader_path'), '/usr/local/lib/libgfortran.5.dylib' (no such file), '/usr/lib/libgfortran.5.dylib' (no such file, not in dyld cache)\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/langchain/lib/python3.12/site-packages/numpy/__init__.py:135\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    132\u001b[39m     msg = \u001b[33m\"\"\"\u001b[39m\u001b[33mError importing numpy: you should not try to import numpy from\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[33m    its source directory; please exit the numpy source tree, and relaunch\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[33m    your python interpreter from there.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01me\u001b[39;00m\n\u001b[32m    137\u001b[39m __all__ = [\n\u001b[32m    138\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mexceptions\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mModuleDeprecationWarning\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mVisibleDeprecationWarning\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    139\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mComplexWarning\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTooHardError\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAxisError\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# mapping of {name: (value, deprecation_msg)}\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Error importing numpy: you should not try to import numpy from\n        its source directory; please exit the numpy source tree, and relaunch\n        your python interpreter from there."
     ]
    }
   ],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e1662d",
   "metadata": {},
   "source": [
    "### python zipfile test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f189abfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/g2/ss03t_z172d6366_2bfxnbpw0000gn/T/ipykernel_37690/1057143016.py\", line 5, in <module>\n",
      "    with zipfile.ZipFile('code_snippets/trl.zip', 'r') as zip_ref:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/seonghwan/miniconda3/lib/python3.12/zipfile/__init__.py\", line 1331, in __init__\n",
      "    self.fp = io.open(file, filemode)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'code_snippets/trl.zip'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile('code_snippets/trl.zip', 'r') as zip_ref:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        zip_ref.extractall(temp_dir)\n",
    "except:\n",
    "    final_str = traceback.format_exc()\n",
    "print(final_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8edd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Traceback (most recent call last): File \"\", line 209, in run File '\n",
      " '\"/usr/local/lib/python3.10/zipfile.py\", line 1251, in init self.fp = '\n",
      " 'io.open(file, filemode) FileNotFoundError: [Errno 2] No such file or '\n",
      " \"directory: '/nfs-root/temp-document/TEMP_DOCUMENT_2029.zip'\")\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(\"Traceback (most recent call last): File \\\"\\\", line 209, in run File \\\"/usr/local/lib/python3.10/zipfile.py\\\", line 1251, in init self.fp = io.open(file, filemode) FileNotFoundError: [Errno 2] No such file or directory: '/nfs-root/temp-document/TEMP_DOCUMENT_2029.zip'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c6f82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
